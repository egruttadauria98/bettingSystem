{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning on football forecasts\n",
    "\n",
    "The aim is to apply Q-Learning methods on top of football match forecasts from https://kickoff.ai in order to understand when is best to bet.\n",
    "\n",
    "The model is both trained and tested using the forecasts from past matches\n",
    "Data are obtained scraping the aforementioned website and then joining with the historical odds available at http://www.football-data.co.uk of the major bookmakers. \n",
    "\n",
    "The model is only trained with (and therefore bets on) basic odds (i.e. 1, X, 2) from B365.\n",
    "Progress can be achieved by betting on the best odds for a given match comparing different bookmakers, but also by considering other, more complicated, bets (i.e. 1X,12,X2,over and under,etc) bearing in mind that this will affect the action space of the model, making it more complicated.\n",
    "\n",
    "Other resources linked to this script:\n",
    "    -  Web Scraping script & joining the data: ...\n",
    "    -  Remake of kickoff.ai model: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def mlp(n_obs, n_action, n_hidden_layer=1, n_neuron_per_layer=32, activation='relu', loss='mse'):\n",
    "    \"\"\" A multi-layer perceptron \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neuron_per_layer, input_dim=n_obs, activation=activation))\n",
    "    for _ in range(n_hidden_layer):\n",
    "        model.add(Dense(n_neuron_per_layer, activation=activation))\n",
    "    model.add(Dense(n_action, activation='linear'))\n",
    "    model.compile(loss=loss, optimizer=Adam())\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "  \n",
    "    def __init__(self, train_data):\n",
    "\n",
    "        # TO DO:\n",
    "        # round up to integer to reduce state space (with percentages in fractions)\n",
    "        # consider betting only on 1\n",
    "        \n",
    "        self.game = train_data[train_data.columns.intersection(['B365H', 'B365D', 'B365A', 'Result'])]\n",
    "        self.train_data = train_data[train_data.columns.intersection(['oddsHome', 'oddsDrawn', 'oddsAway'])]\n",
    "        self.n_step, self.n_bet = self.train_data.shape\n",
    "\n",
    "        # instance attributes\n",
    "        self.cur_step = None\n",
    "        self.profit = None\n",
    "        self.won = None\n",
    "        self.tot = None\n",
    "\n",
    "        # action space\n",
    "        self.action_space = spaces.Discrete(3)   \n",
    "\n",
    "        # observation space: give estimates in order to sample and build scaler\n",
    "    \n",
    "        forecast_max = self.train_data.max(axis=1)\n",
    "        forecast_range = [[0, mx] for mx in forecast_max]\n",
    "\n",
    "        self.observation_space = spaces.MultiDiscrete(forecast_range)\n",
    "        \n",
    "        # seed and start\n",
    "        self._seed()\n",
    "        self._reset()\n",
    "        \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def _reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.profit = 0\n",
    "        self.won = 0\n",
    "        self.tot = 0\n",
    "        self.forecast = self.train_data.iloc[self.cur_step, :]\n",
    "        return self.forecast\n",
    "\n",
    "    def _step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        cur_game = self.game.iloc[self.cur_step, :]\n",
    "        self.cur_step += 1\n",
    "        self.forecast = self.train_data.iloc[self.cur_step, :] # update forecast\n",
    "        reward = self._check_bet(cur_game, action)\n",
    "        self.profit += reward\n",
    "        perc = round((self.won * 100)/self.tot, 2)\n",
    "        done = self.cur_step == self.n_step - 1\n",
    "        info = {'cur_val': ('$' + str(round(self.profit, 2)), str(perc) + '%')}\n",
    "        return self.forecast, reward, done, info\n",
    "\n",
    "    \n",
    "    def _check_bet(self, cur_game, action):\n",
    "        if action == int(cur_game[0]):\n",
    "            reward = cur_game[action] -1\n",
    "            self.won += 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        self.tot += 1\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DQNAgent(object):\n",
    "    \"\"\" A simple Deep Q agent \"\"\"\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = mlp(state_size, action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "\n",
    "    def replay(self, batch_size=32):\n",
    "        \"\"\" vectorized implementation; 30x speed up compared with for loop \"\"\"\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states = np.array([tup[0][0] for tup in minibatch])\n",
    "        actions = np.array([tup[1] for tup in minibatch])\n",
    "        rewards = np.array([tup[2] for tup in minibatch])\n",
    "        next_states = np.array([tup[3][0] for tup in minibatch])\n",
    "        done = np.array([tup[4] for tup in minibatch])\n",
    "\n",
    "        # Q(s', a)\n",
    "        target = rewards + self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
    "        # end state target is reward itself (no lookahead)\n",
    "        target[done] = rewards[done]\n",
    "\n",
    "        # Q(s, a)\n",
    "        target_f = self.model.predict(states)\n",
    "        # make the agent to approximately map the current state to future discounted reward\n",
    "        target_f[range(batch_size), actions] = target\n",
    "\n",
    "        self.model.fit(states, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_data():\n",
    "    data = pd.read_csv('/Users/eliogruttadauria/Desktop/df_pastOdds.csv')\n",
    "    return data\n",
    "\n",
    "def get_scaler(env):\n",
    "    low = [0] * env.n_bet\n",
    "    high = list(env.train_data.max(axis=0))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit([low, high])\n",
    "    return scaler\n",
    "\n",
    "\n",
    "def maybe_make_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "def split_data(data, test_size = 0.2):\n",
    "    #check good split: test must be last data available\n",
    "    cut_point = round(data.shape[0]*test_size)\n",
    "    test_data = data[:cut_point]\n",
    "    train_data = data[cut_point:]\n",
    "    return test_data, train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN (mode = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 1,283\n",
      "Trainable params: 1,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "episode: 1/100, episode end value: ('$106.7', '32.49%')\n",
      "episode: 2/100, episode end value: ('$222.18', '35.02%')\n",
      "episode: 3/100, episode end value: ('$224.75', '32.81%')\n",
      "episode: 4/100, episode end value: ('$243.75', '33.12%')\n",
      "episode: 5/100, episode end value: ('$235.34', '32.49%')\n",
      "episode: 6/100, episode end value: ('$245.72', '32.81%')\n",
      "episode: 7/100, episode end value: ('$245.99', '31.86%')\n",
      "episode: 8/100, episode end value: ('$253.56', '33.44%')\n",
      "episode: 9/100, episode end value: ('$248.0', '32.81%')\n",
      "episode: 10/100, episode end value: ('$245.82', '32.49%')\n",
      "episode: 11/100, episode end value: ('$261.92', '35.96%')\n",
      "episode: 12/100, episode end value: ('$225.63', '32.81%')\n",
      "episode: 13/100, episode end value: ('$251.1', '35.65%')\n",
      "episode: 14/100, episode end value: ('$260.96', '37.85%')\n",
      "episode: 15/100, episode end value: ('$249.86', '35.65%')\n",
      "episode: 16/100, episode end value: ('$229.99', '31.86%')\n",
      "episode: 17/100, episode end value: ('$263.85', '35.65%')\n",
      "episode: 18/100, episode end value: ('$236.2', '32.81%')\n",
      "episode: 19/100, episode end value: ('$258.07', '34.38%')\n",
      "episode: 20/100, episode end value: ('$240.47', '32.18%')\n",
      "episode: 21/100, episode end value: ('$245.55', '33.12%')\n",
      "episode: 22/100, episode end value: ('$244.78', '32.18%')\n",
      "episode: 23/100, episode end value: ('$231.07', '31.55%')\n",
      "episode: 24/100, episode end value: ('$251.65', '34.38%')\n",
      "episode: 25/100, episode end value: ('$257.44', '36.28%')\n",
      "episode: 26/100, episode end value: ('$237.9', '32.49%')\n",
      "episode: 27/100, episode end value: ('$254.64', '34.7%')\n",
      "episode: 28/100, episode end value: ('$241.38', '33.12%')\n",
      "episode: 29/100, episode end value: ('$247.04', '33.44%')\n",
      "episode: 30/100, episode end value: ('$250.31', '33.75%')\n",
      "episode: 31/100, episode end value: ('$239.72', '33.12%')\n",
      "episode: 32/100, episode end value: ('$247.08', '33.44%')\n",
      "episode: 33/100, episode end value: ('$252.69', '34.38%')\n",
      "episode: 34/100, episode end value: ('$251.8', '35.96%')\n",
      "episode: 35/100, episode end value: ('$241.35', '33.12%')\n",
      "episode: 36/100, episode end value: ('$238.42', '33.12%')\n",
      "episode: 37/100, episode end value: ('$251.23', '33.44%')\n",
      "episode: 38/100, episode end value: ('$242.55', '33.75%')\n",
      "episode: 39/100, episode end value: ('$256.62', '34.7%')\n",
      "episode: 40/100, episode end value: ('$259.24', '35.65%')\n",
      "episode: 41/100, episode end value: ('$265.34', '35.96%')\n",
      "episode: 42/100, episode end value: ('$267.4', '35.33%')\n",
      "episode: 43/100, episode end value: ('$260.89', '35.02%')\n",
      "episode: 44/100, episode end value: ('$261.48', '35.02%')\n",
      "episode: 45/100, episode end value: ('$249.23', '32.81%')\n",
      "episode: 46/100, episode end value: ('$239.44', '31.23%')\n",
      "episode: 47/100, episode end value: ('$248.08', '33.44%')\n",
      "episode: 48/100, episode end value: ('$254.8', '33.44%')\n",
      "episode: 49/100, episode end value: ('$252.17', '34.38%')\n",
      "episode: 50/100, episode end value: ('$271.15', '35.65%')\n",
      "episode: 51/100, episode end value: ('$244.45', '33.12%')\n",
      "episode: 52/100, episode end value: ('$272.09', '36.28%')\n",
      "episode: 53/100, episode end value: ('$258.53', '35.96%')\n",
      "episode: 54/100, episode end value: ('$244.18', '34.38%')\n",
      "episode: 55/100, episode end value: ('$251.25', '36.59%')\n",
      "episode: 56/100, episode end value: ('$254.63', '36.28%')\n",
      "episode: 57/100, episode end value: ('$241.94', '33.44%')\n",
      "episode: 58/100, episode end value: ('$246.53', '33.44%')\n",
      "episode: 59/100, episode end value: ('$240.35', '32.49%')\n",
      "episode: 60/100, episode end value: ('$264.07', '34.7%')\n",
      "episode: 61/100, episode end value: ('$263.82', '34.38%')\n",
      "episode: 62/100, episode end value: ('$269.2', '36.59%')\n",
      "episode: 63/100, episode end value: ('$237.03', '32.81%')\n",
      "episode: 64/100, episode end value: ('$265.38', '35.65%')\n",
      "episode: 65/100, episode end value: ('$254.52', '34.7%')\n",
      "episode: 66/100, episode end value: ('$262.11', '36.91%')\n",
      "episode: 67/100, episode end value: ('$274.16', '37.85%')\n",
      "episode: 68/100, episode end value: ('$243.24', '34.38%')\n",
      "episode: 69/100, episode end value: ('$243.94', '33.12%')\n",
      "episode: 70/100, episode end value: ('$257.45', '35.65%')\n",
      "episode: 71/100, episode end value: ('$237.51', '32.18%')\n",
      "episode: 72/100, episode end value: ('$240.31', '32.18%')\n",
      "episode: 73/100, episode end value: ('$242.05', '33.12%')\n",
      "episode: 74/100, episode end value: ('$252.68', '35.02%')\n",
      "episode: 75/100, episode end value: ('$247.12', '33.44%')\n",
      "episode: 76/100, episode end value: ('$257.35', '35.02%')\n",
      "episode: 77/100, episode end value: ('$257.09', '34.7%')\n",
      "episode: 78/100, episode end value: ('$264.52', '36.28%')\n",
      "episode: 79/100, episode end value: ('$262.64', '35.65%')\n",
      "episode: 80/100, episode end value: ('$250.31', '33.12%')\n",
      "episode: 81/100, episode end value: ('$252.98', '34.07%')\n",
      "episode: 82/100, episode end value: ('$261.9', '35.33%')\n",
      "episode: 83/100, episode end value: ('$268.27', '35.33%')\n",
      "episode: 84/100, episode end value: ('$257.65', '34.07%')\n",
      "episode: 85/100, episode end value: ('$247.45', '33.12%')\n",
      "episode: 86/100, episode end value: ('$236.23', '31.86%')\n",
      "episode: 87/100, episode end value: ('$243.05', '32.81%')\n",
      "episode: 88/100, episode end value: ('$245.25', '32.81%')\n",
      "episode: 89/100, episode end value: ('$237.62', '35.65%')\n",
      "episode: 90/100, episode end value: ('$244.95', '33.12%')\n",
      "episode: 91/100, episode end value: ('$245.53', '33.12%')\n",
      "episode: 92/100, episode end value: ('$243.58', '33.12%')\n",
      "episode: 93/100, episode end value: ('$264.14', '34.38%')\n",
      "episode: 94/100, episode end value: ('$252.71', '35.96%')\n",
      "episode: 95/100, episode end value: ('$256.4', '35.33%')\n",
      "episode: 96/100, episode end value: ('$263.08', '35.33%')\n",
      "episode: 97/100, episode end value: ('$270.29', '35.96%')\n",
      "episode: 98/100, episode end value: ('$241.79', '34.38%')\n",
      "episode: 99/100, episode end value: ('$245.67', '34.07%')\n",
      "episode: 100/100, episode end value: ('$245.79', '33.12%')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    mode = 'train'\n",
    "    episode = 100\n",
    "    batch_size = 32\n",
    "        \n",
    "    maybe_make_dir('weights')\n",
    "    maybe_make_dir('portfolio_val')\n",
    "\n",
    "    timestamp = time.strftime('%Y%m%d%H%M')\n",
    "\n",
    "    data = get_data()\n",
    "    test_data, train_data = split_data(data)\n",
    "\n",
    "    env = TradingEnv(train_data)\n",
    "    state_size = 3\n",
    "    action_size = 3 #4\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    scaler = get_scaler(env)\n",
    "\n",
    "    portfolio_value = []\n",
    "    \n",
    "    '''\n",
    "    if mode == 'test':\n",
    "        # remake the env with test data\n",
    "        env = TradingEnv(test_data)\n",
    "        # load trained weights\n",
    "        agent.load(weights)\n",
    "        # when test, the timestamp is same as time when weights was trained\n",
    "        timestamp = re.findall(r'\\d{12}', weights)[0]\n",
    "    '''\n",
    "\n",
    "    for e in range(episode):\n",
    "        state = env._reset()\n",
    "        state = scaler.transform([state])\n",
    "        for time in range(env.n_step):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env._step(action)\n",
    "            next_state = scaler.transform([next_state])\n",
    "            if mode == 'train':\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, episode end value: {}\".format(e + 1, episode, info['cur_val']))\n",
    "                portfolio_value.append(info['cur_val']) # append episode end portfolio value\n",
    "                break\n",
    "            if mode == 'train' and len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "        if mode == 'train' and (e + 1) % 10 == 0:  # checkpoint weights\n",
    "            agent.save('weights/{}-dqn.h5'.format(timestamp))\n",
    "\n",
    "    # save portfolio value history to disk\n",
    "    with open('portfolio_val/{}-{}.p'.format(timestamp, mode), 'wb') as fp:\n",
    "        pickle.dump(portfolio_value, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST (mode = 'test') -- finire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 1,283\n",
      "Trainable params: 1,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-990c864c7148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTradingEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# load trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m# when test, the timestamp is same as time when weights was trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\d{12}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    mode = 'test'\n",
    "    episode = 200\n",
    "    batch_size = 32\n",
    "        \n",
    "    maybe_make_dir('weights')\n",
    "    maybe_make_dir('portfolio_val')\n",
    "\n",
    "    timestamp = time.strftime('%Y%m%d%H%M')\n",
    "\n",
    "    data = get_data()\n",
    "    test_data, train_data = split_data(data)\n",
    "\n",
    "    env = TradingEnv(train_data)\n",
    "    state_size = 3\n",
    "    action_size = 3\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    scaler = get_scaler(env)\n",
    "\n",
    "    portfolio_value = []\n",
    "    \n",
    "    if mode == 'test':\n",
    "        # remake the env with test data\n",
    "        env = TradingEnv(test_data)\n",
    "        # load trained weights\n",
    "        agent.load(weights)\n",
    "        # when test, the timestamp is same as time when weights was trained\n",
    "        timestamp = re.findall(r'\\d{12}', weights)[0]\n",
    "\n",
    "    for e in range(episode):\n",
    "        state = env._reset()\n",
    "        state = scaler.transform([state])\n",
    "        for time in range(env.n_step):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env._step(action)\n",
    "            next_state = scaler.transform([next_state])\n",
    "            if mode == 'train':\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, episode end value: {}\".format(e + 1, episode, info['cur_val']))\n",
    "                portfolio_value.append(info['cur_val']) # append episode end portfolio value\n",
    "                break\n",
    "            if mode == 'train' and len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "        if mode == 'train' and (e + 1) % 10 == 0:  # checkpoint weights\n",
    "            agent.save('weights/{}-dqn.h5'.format(timestamp))\n",
    "\n",
    "    # save portfolio value history to disk\n",
    "    with open('portfolio_val/{}-{}.p'.format(timestamp, mode), 'wb') as fp:\n",
    "        pickle.dump(portfolio_value, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO LIST:\n",
    "\n",
    "    - add 4 action: do not bet\n",
    "    - reduce action with 1 or X2\n",
    "    - repeat each match in opposite direction\n",
    "    - problem in the name of the matches\n",
    "    - different $ for different matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
